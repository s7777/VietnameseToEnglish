{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeoKwYFz5Tqs",
        "outputId": "524425bd-f7a2-4f5e-cb3a-25115700d205"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\t#re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\t#line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\t#line = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\t#line = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\t#line = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\t#line = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = '/content/vie.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-vie.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-vie.pkl\n",
            "[Run] => [Chạy]\n",
            "[Help] => [Giúp tôi với]\n",
            "[Go on] => [Tiếp tục đi]\n",
            "[Hello] => [Chào bạn]\n",
            "[Hurry] => [Nhanh lên nào]\n",
            "[Eat it] => [Ăn đi]\n",
            "[Eat it] => [Ăn nó đi]\n",
            "[Help me] => [Cứu tôi với]\n",
            "[I agree] => [Tôi cũng nghĩ như vậy]\n",
            "[Perfect] => [Hoàn hảo]\n",
            "[We know] => [Chúng tôi biết]\n",
            "[We know] => [Chúng ta biết]\n",
            "[You run] => [Bạn chạy]\n",
            "[Cheer up] => [Đừng có rầu rĩ quá như thế]\n",
            "[He tries] => [Hắn thử]\n",
            "[He tries] => [Anh thử]\n",
            "[Hurry up] => [Thoáng cái chân lên]\n",
            "[I forgot] => [Tôi quên mất rồi]\n",
            "[Im bald] => [Tôi bị hói]\n",
            "[Im busy] => [Tôi đang bận]\n",
            "[Too late] => [Muộn quá]\n",
            "[I hate TV] => [Tôi ghét ti vi]\n",
            "[I laughed] => [Tôi đã cười]\n",
            "[I laughed] => [Tôi cười]\n",
            "[I will go] => [Tôi sẽ đi]\n",
            "[Its cold] => [Lạnh]\n",
            "[Its ours] => [Đó là của chúng tôi]\n",
            "[Its ours] => [Đó là của chúng ta]\n",
            "[She cried] => [Nó đã khóc]\n",
            "[She cried] => [Cô ấy đã khóc]\n",
            "[Sit there] => [Hãy ngồi ở đó]\n",
            "[Whats up] => [Gì thế]\n",
            "[Whats up] => [Có chuyện gì]\n",
            "[Are you OK] => [Bạn có sao không]\n",
            "[Find a job] => [Hãy tìm một công việc đi]\n",
            "[Hurry home] => [Hãy mau về nhà đi]\n",
            "[I can read] => [Tôi đọc được]\n",
            "[I guess so] => [Tôi đoán vậy]\n",
            "[I guess so] => [Tôi đoán thế]\n",
            "[I hate you] => [Tôi ghét anh]\n",
            "[I love you] => [Anh phải lòng em]\n",
            "[I will try] => [Tôi sẽ thử]\n",
            "[Im lonely] => [Tôi cảm thấy cô đơn]\n",
            "[Im so fat] => [Tôi béo quá]\n",
            "[Im so fat] => [Tôi béo vãi]\n",
            "[Its magic] => [Đó là phép thuật]\n",
            "[Let me try] => [Để tôi thử]\n",
            "[No kidding] => [Không đùa chứ]\n",
            "[Time is up] => [Thời gian đã hết]\n",
            "[Time is up] => [Hết giờ]\n",
            "[Time is up] => [Hết thời gian]\n",
            "[Tom winked] => [Tom đã nháy mắt]\n",
            "[Tom winked] => [Tom nháy mắt]\n",
            "[Whos here] => [Ai đây]\n",
            "[Be punctual] => [Hãy đúng giờ]\n",
            "[Dont fight] => [Đừng đánh nhau]\n",
            "[Dont fight] => [Đừng có mà đánh nhau]\n",
            "[I heard you] => [Tôi nghe bạn rồi]\n",
            "[I like both] => [Tôi thích cả hai cái]\n",
            "[I like jazz] => [Tôi thích nhạc jazz]\n",
            "[I like math] => [Tôi thích toán]\n",
            "[I live here] => [Tôi sống ở đây]\n",
            "[I need more] => [Tôi cần thêm]\n",
            "[I need more] => [Tôi cần thêm nữa]\n",
            "[I saw a UFO] => [Tôi nhìn thấy UFO]\n",
            "[I saw a dog] => [Tôi thấy một con chó]\n",
            "[Im nervous] => [Tôi lo lắng]\n",
            "[Im not Tom] => [Tôi không phải Tom]\n",
            "[Im not Tom] => [Tôi không phải là Tom]\n",
            "[Im smashed] => [Tôi đã say rượu]\n",
            "[Im thirsty] => [Tôi khát nước]\n",
            "[It suits me] => [Nó vừa với tôi]\n",
            "[Lets do it] => [Thực hiện thôi]\n",
            "[May I begin] => [Tôi có thể bắt đầu được không]\n",
            "[May I begin] => [Tôi bắt đầu được không ạ]\n",
            "[Nobody came] => [Không ai tới hết]\n",
            "[She is kind] => [Cô ấy là một người tốt bụng]\n",
            "[Tom saw you] => [Tom đã trông thấy bạn]\n",
            "[Tom saw you] => [Tom đã nhìn thấy bạn]\n",
            "[Vote for me] => [Hãy bầu cho tôi]\n",
            "[Vote for me] => [Bầu cho tôi đi]\n",
            "[We miss Tom] => [Chúng tôi nhớ Tom]\n",
            "[Whats that] => [Cái này là cái gì]\n",
            "[Whats this] => [Đây là cái gì]\n",
            "[Youre mine] => [Em là của anh]\n",
            "[Youre mine] => [Anh là của em]\n",
            "[Youre mine] => [Mày là của tao]\n",
            "[Anybody home] => [Có ai ở nhà không]\n",
            "[Anybody home] => [Có ai ở nhà không ạ]\n",
            "[Are you cold] => [Bạn có lạnh không]\n",
            "[Are you cold] => [Bạn lạnh không]\n",
            "[Are you okay] => [Bạn có sao không]\n",
            "[Come help me] => [Hãy đến cứu tôi]\n",
            "[Come with me] => [Đi với tôi]\n",
            "[Dont be shy] => [Đừng ngại]\n",
            "[Dont be shy] => [Đừng ngượng]\n",
            "[Drive faster] => [Lái nhanh hơn đi]\n",
            "[Hang on Tom] => [Chờ chút Tom]\n",
            "[I am at home] => [Tôi đang ở nhà]\n",
            "[I dont know] => [Tôi không biết]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VACc-GX5kZW",
        "outputId": "4be7eb6f-ba82-4163-ebf6-c4f6f4d6a9fa"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-vie.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 6139\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:5000], dataset[1139:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-vie-both.pkl')\n",
        "save_clean_data(train, 'english-vie-train.pkl')\n",
        "save_clean_data(test, 'english-vie-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-vie-both.pkl\n",
            "Saved: english-vie-train.pkl\n",
            "Saved: english-vie-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4DQ72CP58fz",
        "outputId": "b8ad203e-e74b-4f48-d941-ff79f2016eda"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-vie-both.pkl')\n",
        "train = load_clean_sentences('english-vie-train.pkl')\n",
        "test = load_clean_sentences('english-vie-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Vietnam Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Vietnam Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=200, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3500\n",
            "English Max Length: 32\n",
            "Vietnam Vocabulary Size: 2282\n",
            "Vietnam Max Length: 39\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 39, 256)           584192    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 32, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 32, 3500)          899500    \n",
            "=================================================================\n",
            "Total params: 2,534,316\n",
            "Trainable params: 2,534,316\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "79/79 - 94s - loss: 2.4722 - val_loss: 1.4657\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.46571, saving model to model.h5\n",
            "Epoch 2/200\n",
            "79/79 - 87s - loss: 1.4499 - val_loss: 1.4346\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.46571 to 1.43456, saving model to model.h5\n",
            "Epoch 3/200\n",
            "79/79 - 89s - loss: 1.4280 - val_loss: 1.4062\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.43456 to 1.40616, saving model to model.h5\n",
            "Epoch 4/200\n",
            "79/79 - 87s - loss: 1.4152 - val_loss: 1.4030\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.40616 to 1.40301, saving model to model.h5\n",
            "Epoch 5/200\n",
            "79/79 - 87s - loss: 1.3838 - val_loss: 1.3410\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.40301 to 1.34104, saving model to model.h5\n",
            "Epoch 6/200\n",
            "79/79 - 87s - loss: 1.3060 - val_loss: 1.3127\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.34104 to 1.31272, saving model to model.h5\n",
            "Epoch 7/200\n",
            "79/79 - 87s - loss: 1.2910 - val_loss: 1.2784\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.31272 to 1.27842, saving model to model.h5\n",
            "Epoch 8/200\n",
            "79/79 - 87s - loss: 1.2790 - val_loss: 1.2737\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.27842 to 1.27367, saving model to model.h5\n",
            "Epoch 9/200\n",
            "79/79 - 88s - loss: 1.2688 - val_loss: 1.2699\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.27367 to 1.26991, saving model to model.h5\n",
            "Epoch 10/200\n",
            "79/79 - 87s - loss: 1.2618 - val_loss: 1.2577\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.26991 to 1.25768, saving model to model.h5\n",
            "Epoch 11/200\n",
            "79/79 - 87s - loss: 1.2547 - val_loss: 1.2668\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.25768\n",
            "Epoch 12/200\n",
            "79/79 - 87s - loss: 1.2502 - val_loss: 1.2483\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.25768 to 1.24833, saving model to model.h5\n",
            "Epoch 13/200\n",
            "79/79 - 87s - loss: 1.2440 - val_loss: 1.2434\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.24833 to 1.24335, saving model to model.h5\n",
            "Epoch 14/200\n",
            "79/79 - 86s - loss: 1.2380 - val_loss: 1.2584\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.24335\n",
            "Epoch 15/200\n",
            "79/79 - 86s - loss: 1.2326 - val_loss: 1.2352\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.24335 to 1.23520, saving model to model.h5\n",
            "Epoch 16/200\n",
            "79/79 - 86s - loss: 1.2267 - val_loss: 1.2539\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.23520\n",
            "Epoch 17/200\n",
            "79/79 - 86s - loss: 1.2235 - val_loss: 1.2280\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.23520 to 1.22799, saving model to model.h5\n",
            "Epoch 18/200\n",
            "79/79 - 86s - loss: 1.2163 - val_loss: 1.2240\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.22799 to 1.22400, saving model to model.h5\n",
            "Epoch 19/200\n",
            "79/79 - 86s - loss: 1.2098 - val_loss: 1.2242\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.22400\n",
            "Epoch 20/200\n",
            "79/79 - 86s - loss: 1.2077 - val_loss: 1.2169\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.22400 to 1.21687, saving model to model.h5\n",
            "Epoch 21/200\n",
            "79/79 - 87s - loss: 1.2029 - val_loss: 1.2180\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.21687\n",
            "Epoch 22/200\n",
            "79/79 - 87s - loss: 1.1972 - val_loss: 1.2116\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.21687 to 1.21156, saving model to model.h5\n",
            "Epoch 23/200\n",
            "79/79 - 86s - loss: 1.1896 - val_loss: 1.2079\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.21156 to 1.20789, saving model to model.h5\n",
            "Epoch 24/200\n",
            "79/79 - 86s - loss: 1.1890 - val_loss: 1.2054\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.20789 to 1.20540, saving model to model.h5\n",
            "Epoch 25/200\n",
            "79/79 - 86s - loss: 1.1812 - val_loss: 1.2097\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.20540\n",
            "Epoch 26/200\n",
            "79/79 - 86s - loss: 1.1776 - val_loss: 1.1944\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.20540 to 1.19435, saving model to model.h5\n",
            "Epoch 27/200\n",
            "79/79 - 86s - loss: 1.1705 - val_loss: 1.1931\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.19435 to 1.19307, saving model to model.h5\n",
            "Epoch 28/200\n",
            "79/79 - 86s - loss: 1.1618 - val_loss: 1.1946\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.19307\n",
            "Epoch 29/200\n",
            "79/79 - 86s - loss: 1.1543 - val_loss: 1.1989\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.19307\n",
            "Epoch 30/200\n",
            "79/79 - 87s - loss: 1.1418 - val_loss: 1.1631\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.19307 to 1.16312, saving model to model.h5\n",
            "Epoch 31/200\n",
            "79/79 - 86s - loss: 1.1250 - val_loss: 1.1524\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.16312 to 1.15245, saving model to model.h5\n",
            "Epoch 32/200\n",
            "79/79 - 86s - loss: 1.1122 - val_loss: 1.1362\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.15245 to 1.13622, saving model to model.h5\n",
            "Epoch 33/200\n",
            "79/79 - 87s - loss: 1.0942 - val_loss: 1.1187\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.13622 to 1.11871, saving model to model.h5\n",
            "Epoch 34/200\n",
            "79/79 - 86s - loss: 1.0752 - val_loss: 1.1039\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.11871 to 1.10390, saving model to model.h5\n",
            "Epoch 35/200\n",
            "79/79 - 86s - loss: 1.0591 - val_loss: 1.0999\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.10390 to 1.09986, saving model to model.h5\n",
            "Epoch 36/200\n",
            "79/79 - 86s - loss: 1.0425 - val_loss: 1.0877\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.09986 to 1.08766, saving model to model.h5\n",
            "Epoch 37/200\n",
            "79/79 - 86s - loss: 1.0260 - val_loss: 1.0565\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.08766 to 1.05651, saving model to model.h5\n",
            "Epoch 38/200\n",
            "79/79 - 86s - loss: 1.0047 - val_loss: 1.0452\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.05651 to 1.04517, saving model to model.h5\n",
            "Epoch 39/200\n",
            "79/79 - 86s - loss: 0.9849 - val_loss: 1.0202\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.04517 to 1.02019, saving model to model.h5\n",
            "Epoch 40/200\n",
            "79/79 - 86s - loss: 0.9638 - val_loss: 1.0031\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.02019 to 1.00307, saving model to model.h5\n",
            "Epoch 41/200\n",
            "79/79 - 86s - loss: 0.9466 - val_loss: 0.9932\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.00307 to 0.99316, saving model to model.h5\n",
            "Epoch 42/200\n",
            "79/79 - 87s - loss: 0.9286 - val_loss: 0.9879\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.99316 to 0.98789, saving model to model.h5\n",
            "Epoch 43/200\n",
            "79/79 - 86s - loss: 0.9103 - val_loss: 0.9563\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.98789 to 0.95633, saving model to model.h5\n",
            "Epoch 44/200\n",
            "79/79 - 87s - loss: 0.8887 - val_loss: 0.9377\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.95633 to 0.93766, saving model to model.h5\n",
            "Epoch 45/200\n",
            "79/79 - 86s - loss: 0.8667 - val_loss: 0.9208\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.93766 to 0.92081, saving model to model.h5\n",
            "Epoch 46/200\n",
            "79/79 - 87s - loss: 0.8504 - val_loss: 0.9258\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.92081\n",
            "Epoch 47/200\n",
            "79/79 - 86s - loss: 0.8328 - val_loss: 0.8907\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.92081 to 0.89072, saving model to model.h5\n",
            "Epoch 48/200\n",
            "79/79 - 87s - loss: 0.8144 - val_loss: 0.8811\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.89072 to 0.88115, saving model to model.h5\n",
            "Epoch 49/200\n",
            "79/79 - 87s - loss: 0.7932 - val_loss: 0.8596\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.88115 to 0.85957, saving model to model.h5\n",
            "Epoch 50/200\n",
            "79/79 - 87s - loss: 0.7746 - val_loss: 0.8451\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.85957 to 0.84509, saving model to model.h5\n",
            "Epoch 51/200\n",
            "79/79 - 86s - loss: 0.7549 - val_loss: 0.8322\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.84509 to 0.83220, saving model to model.h5\n",
            "Epoch 52/200\n",
            "79/79 - 87s - loss: 0.7352 - val_loss: 0.8131\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.83220 to 0.81312, saving model to model.h5\n",
            "Epoch 53/200\n",
            "79/79 - 86s - loss: 0.7176 - val_loss: 0.7997\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.81312 to 0.79971, saving model to model.h5\n",
            "Epoch 54/200\n",
            "79/79 - 86s - loss: 0.7021 - val_loss: 0.7876\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.79971 to 0.78757, saving model to model.h5\n",
            "Epoch 55/200\n",
            "79/79 - 86s - loss: 0.6873 - val_loss: 0.7729\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.78757 to 0.77289, saving model to model.h5\n",
            "Epoch 56/200\n",
            "79/79 - 87s - loss: 0.6705 - val_loss: 0.7629\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.77289 to 0.76291, saving model to model.h5\n",
            "Epoch 57/200\n",
            "79/79 - 86s - loss: 0.6528 - val_loss: 0.7469\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.76291 to 0.74687, saving model to model.h5\n",
            "Epoch 58/200\n",
            "79/79 - 87s - loss: 0.6355 - val_loss: 0.7432\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.74687 to 0.74321, saving model to model.h5\n",
            "Epoch 59/200\n",
            "79/79 - 87s - loss: 0.6233 - val_loss: 0.7292\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.74321 to 0.72921, saving model to model.h5\n",
            "Epoch 60/200\n",
            "79/79 - 87s - loss: 0.6060 - val_loss: 0.7229\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.72921 to 0.72287, saving model to model.h5\n",
            "Epoch 61/200\n",
            "79/79 - 86s - loss: 0.5916 - val_loss: 0.6987\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.72287 to 0.69868, saving model to model.h5\n",
            "Epoch 62/200\n",
            "79/79 - 86s - loss: 0.5715 - val_loss: 0.6858\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.69868 to 0.68577, saving model to model.h5\n",
            "Epoch 63/200\n",
            "79/79 - 87s - loss: 0.5537 - val_loss: 0.6750\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.68577 to 0.67503, saving model to model.h5\n",
            "Epoch 64/200\n",
            "79/79 - 86s - loss: 0.5444 - val_loss: 0.6668\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.67503 to 0.66680, saving model to model.h5\n",
            "Epoch 65/200\n",
            "79/79 - 86s - loss: 0.5298 - val_loss: 0.6580\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.66680 to 0.65804, saving model to model.h5\n",
            "Epoch 66/200\n",
            "79/79 - 86s - loss: 0.5150 - val_loss: 0.6542\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.65804 to 0.65423, saving model to model.h5\n",
            "Epoch 67/200\n",
            "79/79 - 86s - loss: 0.5043 - val_loss: 0.6353\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.65423 to 0.63525, saving model to model.h5\n",
            "Epoch 68/200\n",
            "79/79 - 86s - loss: 0.4903 - val_loss: 0.6325\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.63525 to 0.63254, saving model to model.h5\n",
            "Epoch 69/200\n",
            "79/79 - 86s - loss: 0.4796 - val_loss: 0.6192\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.63254 to 0.61922, saving model to model.h5\n",
            "Epoch 70/200\n",
            "79/79 - 87s - loss: 0.4607 - val_loss: 0.6072\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.61922 to 0.60717, saving model to model.h5\n",
            "Epoch 71/200\n",
            "79/79 - 87s - loss: 0.4486 - val_loss: 0.6050\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.60717 to 0.60501, saving model to model.h5\n",
            "Epoch 72/200\n",
            "79/79 - 87s - loss: 0.4433 - val_loss: 0.5928\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.60501 to 0.59281, saving model to model.h5\n",
            "Epoch 73/200\n",
            "79/79 - 86s - loss: 0.4260 - val_loss: 0.5786\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.59281 to 0.57865, saving model to model.h5\n",
            "Epoch 74/200\n",
            "79/79 - 86s - loss: 0.4139 - val_loss: 0.5708\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.57865 to 0.57081, saving model to model.h5\n",
            "Epoch 75/200\n",
            "79/79 - 86s - loss: 0.4022 - val_loss: 0.5642\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.57081 to 0.56420, saving model to model.h5\n",
            "Epoch 76/200\n",
            "79/79 - 87s - loss: 0.3894 - val_loss: 0.5524\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.56420 to 0.55239, saving model to model.h5\n",
            "Epoch 77/200\n",
            "79/79 - 86s - loss: 0.3776 - val_loss: 0.5471\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.55239 to 0.54705, saving model to model.h5\n",
            "Epoch 78/200\n",
            "79/79 - 87s - loss: 0.3659 - val_loss: 0.5420\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.54705 to 0.54200, saving model to model.h5\n",
            "Epoch 79/200\n",
            "79/79 - 86s - loss: 0.3606 - val_loss: 0.5315\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.54200 to 0.53150, saving model to model.h5\n",
            "Epoch 80/200\n",
            "79/79 - 86s - loss: 0.3480 - val_loss: 0.5263\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.53150 to 0.52634, saving model to model.h5\n",
            "Epoch 81/200\n",
            "79/79 - 87s - loss: 0.3379 - val_loss: 0.5191\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.52634 to 0.51909, saving model to model.h5\n",
            "Epoch 82/200\n",
            "79/79 - 87s - loss: 0.3244 - val_loss: 0.5107\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.51909 to 0.51069, saving model to model.h5\n",
            "Epoch 83/200\n",
            "79/79 - 87s - loss: 0.3141 - val_loss: 0.5021\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.51069 to 0.50214, saving model to model.h5\n",
            "Epoch 84/200\n",
            "79/79 - 87s - loss: 0.3072 - val_loss: 0.4977\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.50214 to 0.49774, saving model to model.h5\n",
            "Epoch 85/200\n",
            "79/79 - 87s - loss: 0.2972 - val_loss: 0.4942\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.49774 to 0.49422, saving model to model.h5\n",
            "Epoch 86/200\n",
            "79/79 - 87s - loss: 0.2906 - val_loss: 0.4956\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.49422\n",
            "Epoch 87/200\n",
            "79/79 - 87s - loss: 0.2825 - val_loss: 0.4770\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.49422 to 0.47702, saving model to model.h5\n",
            "Epoch 88/200\n",
            "79/79 - 87s - loss: 0.2684 - val_loss: 0.4705\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.47702 to 0.47052, saving model to model.h5\n",
            "Epoch 89/200\n",
            "79/79 - 86s - loss: 0.2617 - val_loss: 0.4727\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.47052\n",
            "Epoch 90/200\n",
            "79/79 - 87s - loss: 0.2606 - val_loss: 0.4676\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.47052 to 0.46757, saving model to model.h5\n",
            "Epoch 91/200\n",
            "79/79 - 87s - loss: 0.2500 - val_loss: 0.4566\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.46757 to 0.45660, saving model to model.h5\n",
            "Epoch 92/200\n",
            "79/79 - 87s - loss: 0.2411 - val_loss: 0.4517\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.45660 to 0.45171, saving model to model.h5\n",
            "Epoch 93/200\n",
            "79/79 - 87s - loss: 0.2336 - val_loss: 0.4496\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.45171 to 0.44965, saving model to model.h5\n",
            "Epoch 94/200\n",
            "79/79 - 86s - loss: 0.2232 - val_loss: 0.4410\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.44965 to 0.44100, saving model to model.h5\n",
            "Epoch 95/200\n",
            "79/79 - 87s - loss: 0.2168 - val_loss: 0.4360\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.44100 to 0.43599, saving model to model.h5\n",
            "Epoch 96/200\n",
            "79/79 - 86s - loss: 0.2097 - val_loss: 0.4320\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.43599 to 0.43200, saving model to model.h5\n",
            "Epoch 97/200\n",
            "79/79 - 86s - loss: 0.2044 - val_loss: 0.4291\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.43200 to 0.42911, saving model to model.h5\n",
            "Epoch 98/200\n",
            "79/79 - 86s - loss: 0.1980 - val_loss: 0.4323\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.42911\n",
            "Epoch 99/200\n",
            "79/79 - 86s - loss: 0.1964 - val_loss: 0.4217\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.42911 to 0.42167, saving model to model.h5\n",
            "Epoch 100/200\n",
            "79/79 - 86s - loss: 0.1867 - val_loss: 0.4173\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.42167 to 0.41730, saving model to model.h5\n",
            "Epoch 101/200\n",
            "79/79 - 86s - loss: 0.1766 - val_loss: 0.4111\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.41730 to 0.41105, saving model to model.h5\n",
            "Epoch 102/200\n",
            "79/79 - 86s - loss: 0.1679 - val_loss: 0.4110\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.41105 to 0.41103, saving model to model.h5\n",
            "Epoch 103/200\n",
            "79/79 - 86s - loss: 0.1655 - val_loss: 0.4135\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.41103\n",
            "Epoch 104/200\n",
            "79/79 - 86s - loss: 0.1683 - val_loss: 0.4107\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.41103 to 0.41074, saving model to model.h5\n",
            "Epoch 105/200\n",
            "79/79 - 86s - loss: 0.1609 - val_loss: 0.4029\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.41074 to 0.40290, saving model to model.h5\n",
            "Epoch 106/200\n",
            "79/79 - 86s - loss: 0.1516 - val_loss: 0.4037\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.40290\n",
            "Epoch 107/200\n",
            "79/79 - 86s - loss: 0.1490 - val_loss: 0.4027\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.40290 to 0.40274, saving model to model.h5\n",
            "Epoch 108/200\n",
            "79/79 - 86s - loss: 0.1427 - val_loss: 0.3922\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.40274 to 0.39223, saving model to model.h5\n",
            "Epoch 109/200\n",
            "79/79 - 87s - loss: 0.1361 - val_loss: 0.3935\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.39223\n",
            "Epoch 110/200\n",
            "79/79 - 86s - loss: 0.1295 - val_loss: 0.3848\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.39223 to 0.38476, saving model to model.h5\n",
            "Epoch 111/200\n",
            "79/79 - 87s - loss: 0.1228 - val_loss: 0.3857\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.38476\n",
            "Epoch 112/200\n",
            "79/79 - 87s - loss: 0.1191 - val_loss: 0.3788\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.38476 to 0.37882, saving model to model.h5\n",
            "Epoch 113/200\n",
            "79/79 - 87s - loss: 0.1174 - val_loss: 0.3884\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.37882\n",
            "Epoch 114/200\n",
            "79/79 - 86s - loss: 0.1200 - val_loss: 0.3820\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.37882\n",
            "Epoch 115/200\n",
            "79/79 - 86s - loss: 0.1144 - val_loss: 0.3823\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.37882\n",
            "Epoch 116/200\n",
            "79/79 - 86s - loss: 0.1108 - val_loss: 0.3805\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.37882\n",
            "Epoch 117/200\n",
            "79/79 - 87s - loss: 0.1097 - val_loss: 0.3771\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.37882 to 0.37708, saving model to model.h5\n",
            "Epoch 118/200\n",
            "79/79 - 87s - loss: 0.1035 - val_loss: 0.3751\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.37708 to 0.37508, saving model to model.h5\n",
            "Epoch 119/200\n",
            "79/79 - 87s - loss: 0.0972 - val_loss: 0.3689\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.37508 to 0.36893, saving model to model.h5\n",
            "Epoch 120/200\n",
            "79/79 - 87s - loss: 0.0944 - val_loss: 0.3694\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.36893\n",
            "Epoch 121/200\n",
            "79/79 - 87s - loss: 0.0926 - val_loss: 0.3716\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.36893\n",
            "Epoch 122/200\n",
            "79/79 - 87s - loss: 0.0917 - val_loss: 0.3690\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.36893\n",
            "Epoch 123/200\n",
            "79/79 - 87s - loss: 0.0889 - val_loss: 0.3673\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.36893 to 0.36733, saving model to model.h5\n",
            "Epoch 124/200\n",
            "79/79 - 87s - loss: 0.0897 - val_loss: 0.3700\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.36733\n",
            "Epoch 125/200\n",
            "79/79 - 87s - loss: 0.0869 - val_loss: 0.3664\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.36733 to 0.36642, saving model to model.h5\n",
            "Epoch 126/200\n",
            "79/79 - 87s - loss: 0.0832 - val_loss: 0.3654\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.36642 to 0.36538, saving model to model.h5\n",
            "Epoch 127/200\n",
            "79/79 - 87s - loss: 0.0790 - val_loss: 0.3677\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.36538\n",
            "Epoch 128/200\n",
            "79/79 - 87s - loss: 0.0791 - val_loss: 0.3617\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.36538 to 0.36172, saving model to model.h5\n",
            "Epoch 129/200\n",
            "79/79 - 87s - loss: 0.0731 - val_loss: 0.3606\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.36172 to 0.36060, saving model to model.h5\n",
            "Epoch 130/200\n",
            "79/79 - 87s - loss: 0.0706 - val_loss: 0.3619\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.36060\n",
            "Epoch 131/200\n",
            "79/79 - 88s - loss: 0.0683 - val_loss: 0.3662\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.36060\n",
            "Epoch 132/200\n",
            "79/79 - 87s - loss: 0.0848 - val_loss: 0.3661\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.36060\n",
            "Epoch 133/200\n",
            "79/79 - 88s - loss: 0.0766 - val_loss: 0.3686\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.36060\n",
            "Epoch 134/200\n",
            "79/79 - 88s - loss: 0.0719 - val_loss: 0.3591\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.36060 to 0.35912, saving model to model.h5\n",
            "Epoch 135/200\n",
            "79/79 - 88s - loss: 0.0637 - val_loss: 0.3600\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.35912\n",
            "Epoch 136/200\n",
            "79/79 - 87s - loss: 0.0658 - val_loss: 0.3671\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.35912\n",
            "Epoch 137/200\n",
            "79/79 - 87s - loss: 0.0667 - val_loss: 0.3580\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.35912 to 0.35802, saving model to model.h5\n",
            "Epoch 138/200\n",
            "79/79 - 87s - loss: 0.0595 - val_loss: 0.3563\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.35802 to 0.35628, saving model to model.h5\n",
            "Epoch 139/200\n",
            "79/79 - 87s - loss: 0.0554 - val_loss: 0.3540\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.35628 to 0.35400, saving model to model.h5\n",
            "Epoch 140/200\n",
            "79/79 - 87s - loss: 0.0532 - val_loss: 0.3532\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.35400 to 0.35318, saving model to model.h5\n",
            "Epoch 141/200\n",
            "79/79 - 87s - loss: 0.0516 - val_loss: 0.3531\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.35318 to 0.35305, saving model to model.h5\n",
            "Epoch 142/200\n",
            "79/79 - 87s - loss: 0.0494 - val_loss: 0.3524\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.35305 to 0.35241, saving model to model.h5\n",
            "Epoch 143/200\n",
            "79/79 - 87s - loss: 0.0488 - val_loss: 0.3531\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.35241\n",
            "Epoch 144/200\n",
            "79/79 - 87s - loss: 0.0509 - val_loss: 0.3553\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.35241\n",
            "Epoch 145/200\n",
            "79/79 - 88s - loss: 0.0493 - val_loss: 0.3574\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.35241\n",
            "Epoch 146/200\n",
            "79/79 - 87s - loss: 0.0520 - val_loss: 0.3576\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.35241\n",
            "Epoch 147/200\n",
            "79/79 - 87s - loss: 0.0556 - val_loss: 0.3638\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.35241\n",
            "Epoch 148/200\n",
            "79/79 - 87s - loss: 0.0663 - val_loss: 0.3684\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.35241\n",
            "Epoch 149/200\n",
            "79/79 - 87s - loss: 0.0629 - val_loss: 0.3667\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.35241\n",
            "Epoch 150/200\n",
            "79/79 - 87s - loss: 0.0623 - val_loss: 0.3629\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.35241\n",
            "Epoch 151/200\n",
            "79/79 - 88s - loss: 0.0636 - val_loss: 0.3615\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.35241\n",
            "Epoch 152/200\n",
            "79/79 - 87s - loss: 0.0568 - val_loss: 0.3565\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.35241\n",
            "Epoch 153/200\n",
            "79/79 - 87s - loss: 0.0459 - val_loss: 0.3520\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.35241 to 0.35197, saving model to model.h5\n",
            "Epoch 154/200\n",
            "79/79 - 87s - loss: 0.0393 - val_loss: 0.3512\n",
            "\n",
            "Epoch 00154: val_loss improved from 0.35197 to 0.35116, saving model to model.h5\n",
            "Epoch 155/200\n",
            "79/79 - 87s - loss: 0.0365 - val_loss: 0.3492\n",
            "\n",
            "Epoch 00155: val_loss improved from 0.35116 to 0.34917, saving model to model.h5\n",
            "Epoch 156/200\n",
            "79/79 - 87s - loss: 0.0352 - val_loss: 0.3499\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.34917\n",
            "Epoch 157/200\n",
            "79/79 - 87s - loss: 0.0352 - val_loss: 0.3500\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.34917\n",
            "Epoch 158/200\n",
            "79/79 - 87s - loss: 0.0359 - val_loss: 0.3568\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.34917\n",
            "Epoch 159/200\n",
            "79/79 - 87s - loss: 0.0371 - val_loss: 0.3527\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.34917\n",
            "Epoch 160/200\n",
            "79/79 - 87s - loss: 0.0385 - val_loss: 0.3561\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.34917\n",
            "Epoch 161/200\n",
            "79/79 - 87s - loss: 0.0407 - val_loss: 0.3593\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.34917\n",
            "Epoch 162/200\n",
            "79/79 - 87s - loss: 0.0443 - val_loss: 0.3654\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.34917\n",
            "Epoch 163/200\n",
            "79/79 - 87s - loss: 0.0436 - val_loss: 0.3608\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.34917\n",
            "Epoch 164/200\n",
            "79/79 - 86s - loss: 0.0438 - val_loss: 0.3687\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.34917\n",
            "Epoch 165/200\n",
            "79/79 - 87s - loss: 0.0459 - val_loss: 0.3616\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.34917\n",
            "Epoch 166/200\n",
            "79/79 - 87s - loss: 0.0453 - val_loss: 0.3636\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.34917\n",
            "Epoch 167/200\n",
            "79/79 - 87s - loss: 0.0418 - val_loss: 0.3602\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.34917\n",
            "Epoch 168/200\n",
            "79/79 - 87s - loss: 0.0389 - val_loss: 0.3591\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.34917\n",
            "Epoch 169/200\n",
            "79/79 - 87s - loss: 0.0370 - val_loss: 0.3576\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.34917\n",
            "Epoch 170/200\n",
            "79/79 - 87s - loss: 0.0356 - val_loss: 0.3568\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.34917\n",
            "Epoch 171/200\n",
            "79/79 - 87s - loss: 0.0360 - val_loss: 0.3618\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.34917\n",
            "Epoch 172/200\n",
            "79/79 - 87s - loss: 0.0401 - val_loss: 0.3619\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.34917\n",
            "Epoch 173/200\n",
            "79/79 - 87s - loss: 0.0370 - val_loss: 0.3594\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.34917\n",
            "Epoch 174/200\n",
            "79/79 - 87s - loss: 0.0355 - val_loss: 0.3599\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.34917\n",
            "Epoch 175/200\n",
            "79/79 - 87s - loss: 0.0368 - val_loss: 0.3578\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.34917\n",
            "Epoch 176/200\n",
            "79/79 - 87s - loss: 0.0331 - val_loss: 0.3596\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.34917\n",
            "Epoch 177/200\n",
            "79/79 - 87s - loss: 0.0325 - val_loss: 0.3570\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.34917\n",
            "Epoch 178/200\n",
            "79/79 - 87s - loss: 0.0309 - val_loss: 0.3584\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.34917\n",
            "Epoch 179/200\n",
            "79/79 - 87s - loss: 0.0370 - val_loss: 0.3662\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.34917\n",
            "Epoch 180/200\n",
            "79/79 - 87s - loss: 0.0411 - val_loss: 0.3685\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.34917\n",
            "Epoch 181/200\n",
            "79/79 - 87s - loss: 0.0402 - val_loss: 0.3635\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.34917\n",
            "Epoch 182/200\n",
            "79/79 - 87s - loss: 0.0384 - val_loss: 0.3666\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.34917\n",
            "Epoch 183/200\n",
            "79/79 - 87s - loss: 0.0363 - val_loss: 0.3627\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.34917\n",
            "Epoch 184/200\n",
            "79/79 - 86s - loss: 0.0341 - val_loss: 0.3610\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.34917\n",
            "Epoch 185/200\n",
            "79/79 - 86s - loss: 0.0317 - val_loss: 0.3597\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.34917\n",
            "Epoch 186/200\n",
            "79/79 - 86s - loss: 0.0312 - val_loss: 0.3615\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.34917\n",
            "Epoch 187/200\n",
            "79/79 - 86s - loss: 0.0285 - val_loss: 0.3576\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.34917\n",
            "Epoch 188/200\n",
            "79/79 - 86s - loss: 0.0257 - val_loss: 0.3568\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.34917\n",
            "Epoch 189/200\n",
            "79/79 - 86s - loss: 0.0244 - val_loss: 0.3593\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.34917\n",
            "Epoch 190/200\n",
            "79/79 - 87s - loss: 0.0256 - val_loss: 0.3586\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.34917\n",
            "Epoch 191/200\n",
            "79/79 - 86s - loss: 0.0254 - val_loss: 0.3595\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.34917\n",
            "Epoch 192/200\n",
            "79/79 - 86s - loss: 0.0255 - val_loss: 0.3620\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.34917\n",
            "Epoch 193/200\n",
            "79/79 - 86s - loss: 0.0263 - val_loss: 0.3621\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.34917\n",
            "Epoch 194/200\n",
            "79/79 - 87s - loss: 0.0272 - val_loss: 0.3622\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.34917\n",
            "Epoch 195/200\n",
            "79/79 - 87s - loss: 0.0313 - val_loss: 0.3693\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.34917\n",
            "Epoch 196/200\n",
            "79/79 - 87s - loss: 0.0421 - val_loss: 0.3849\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.34917\n",
            "Epoch 197/200\n",
            "79/79 - 87s - loss: 0.0522 - val_loss: 0.3876\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.34917\n",
            "Epoch 198/200\n",
            "79/79 - 87s - loss: 0.0561 - val_loss: 0.3772\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.34917\n",
            "Epoch 199/200\n",
            "79/79 - 87s - loss: 0.0523 - val_loss: 0.3770\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.34917\n",
            "Epoch 200/200\n",
            "79/79 - 87s - loss: 0.0416 - val_loss: 0.3685\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.34917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f53879e9898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEz9UHL3VM4X",
        "outputId": "4789b718-1f0c-44e1-bdce-73c5ded81837"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-vie-both.pkl')\n",
        "train = load_clean_sentences('english-vie-train.pkl')\n",
        "test = load_clean_sentences('english-vie-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[Có 13 hành khách đã nhập viện.], target=[Thirteen passengers were hospitalized.], predicted=[thirteen passengers were hospitalized]\n",
            "src=[Bạn đã nói dối chúng tôi.], target=[You lied to us.], predicted=[we lied to us]\n",
            "src=[Tom chỉ ăn thực phẩm hữu cơ.], target=[Tom only eats organic food.], predicted=[tom only eats organic food]\n",
            "src=[Tôi sinh ra ở Osaka vào năm 1977.], target=[I was born in Osaka in 1977.], predicted=[i was born in osaka in 1977]\n",
            "src=[Bà ấy tự sưởi ấm bằng lửa.], target=[She warmed herself by the fire.], predicted=[she warmed herself by the fire]\n",
            "src=[Tôi không có phản hồi nào từ bất kỳ ai.], target=[I got no response from anyone.], predicted=[i got no response for anyone]\n",
            "src=[Đó là loại cá gì?], target=[What kind of fish is that?], predicted=[what kind of fish is that]\n",
            "src=[Chúng tôi đã không để ý tới thời gian.], target=[We weren't aware of the time.], predicted=[we weren't aware of the time]\n",
            "src=[Tom chạy sang giúp Mary.], target=[Tom ran over to help Mary.], predicted=[tom ran over over to mary]\n",
            "src=[Tôi không muốn vào tù.], target=[I don't want to go to jail.], predicted=[i don't want to go to to jail]\n",
            "BLEU-1: 0.488834\n",
            "BLEU-2: 0.378002\n",
            "BLEU-3: 0.323624\n",
            "BLEU-4: 0.210902\n",
            "test\n",
            "src=[Cảnh từ trên đỉnh núi khiến tôi phải nín thở.], target=[The view from the top of the mountain took my breath away.], predicted=[the view the the the the the mountain took from breath]\n",
            "src=[Bạn phải biết là việc đó thỉnh thoảng vẫn xảy ra.], target=[You have to expect that to happen once in a while.], predicted=[you do you expect that to happen happen in a while]\n",
            "src=[Làm ơn hãy đóng cửa.], target=[Please close the door.], predicted=[please the door door]\n",
            "src=[Tom đã ném điện thoại của Mary vào bể bơi.], target=[Tom threw Mary's phone into the pool.], predicted=[tom threw mary's phone into the pool]\n",
            "src=[Tôi chuẩn bị đi mua vài chai rượu cho bữa tối.], target=[I'm going to buy a few bottles of wine for dinner.], predicted=[i going to to to bottles bottles bottles wine for dinner]\n",
            "src=[Cái đó tùy thuộc vào văn cảnh.], target=[It depends on the context.], predicted=[it depends the the context]\n",
            "src=[Sâu bọ có lớp vỏ cứng.], target=[Insects have a hard skin.], predicted=[insects have a hard skin]\n",
            "src=[Bạn hãy cho cuốn sách cho người nào muốn.], target=[Give the book to whomever wants it.], predicted=[do the book book whomever to]\n",
            "src=[Tôi không nhớ gì về điều đó.], target=[I don't remember anything about that.], predicted=[i don't remember anything about that]\n",
            "src=[Có ai nhìn thấy những cái chìa khóa của tôi không?], target=[Has anyone seen my keys?], predicted=[has anyone seen my keys]\n",
            "BLEU-1: 0.420026\n",
            "BLEU-2: 0.316161\n",
            "BLEU-3: 0.271970\n",
            "BLEU-4: 0.172822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgcNi6oBbBB6",
        "outputId": "53479640-2c39-48b7-cd6e-97c5b39a9a2b"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-vie-both.pkl')\n",
        "train = load_clean_sentences('english-vie-train.pkl')\n",
        "test = load_clean_sentences('english-vie-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[Mơ ước của Tom là đi du lịch vòng quanh thế giới với Mary], target=[Toms dream is to travel around the world with Mary], predicted=[toms was sitting for the the the world with mary]\n",
            "src=[Nói tôi nghe tại sao anh lại không đi Boston với chúng tôi], target=[Tell me why you arent planning on going to Boston with us], predicted=[how did why you wasnt planning to to to boston with]\n",
            "src=[Tôi muốn Tom bị bắt], target=[I want Tom arrested], predicted=[i want tom arrested]\n",
            "src=[Đêm cũng dài quá ha], target=[Well the night is quite long isnt it], predicted=[the the is is quite long isnt]\n",
            "src=[Tôi ước gì tôi có thêm thời gian], target=[I wish I had more time], predicted=[i wish i had more time]\n",
            "src=[Năm sau tôi sẽ quay trở lại Boston], target=[Next year Ill return to Boston], predicted=[ill year ill return to boston]\n",
            "src=[Bọn họ đã lấy nhau rồi], target=[They were married], predicted=[they were married]\n",
            "src=[Đề xuất đó đã được thông qua], target=[The proposal went through], predicted=[the proposal went through]\n",
            "src=[Tất cả chúng tôi cùng đứng dậy], target=[We all stood up at once], predicted=[we all stood at the time]\n",
            "src=[Mặc dù mắt của Tom vẫn còn nước nó đã bắt đầu cười], target=[Even though Tom still had tears in his eyes he began to smile], predicted=[tom though though to though tears tears and working he began to smile]\n",
            "BLEU-1: 0.631006\n",
            "BLEU-2: 0.524353\n",
            "BLEU-3: 0.471760\n",
            "BLEU-4: 0.356621\n",
            "test\n",
            "src=[Cô ấy đã nhìn anh ấy với sự căm ghét], target=[She looked at him with hatred], predicted=[she looked him him with hatred]\n",
            "src=[Bạn cần phải học thuộc lòng càng nhiều từ tiếng Anh càng tốt], target=[You should memorize as many English words as possible], predicted=[you must memorize to to english as possible]\n",
            "src=[Giúp tôi với], target=[Help], predicted=[help]\n",
            "src=[Tom đã mất tích gần ba tuần], target=[Tom has been missing for almost three weeks], predicted=[tom has been missing for almost three weeks]\n",
            "src=[Cô ấy đã từng là một người phụ nữ quyến rũ], target=[She was a charming woman], predicted=[she was a charming woman]\n",
            "src=[Tom gần như không bao giờ sai], target=[Tom is almost never wrong], predicted=[tom is almost never wrong]\n",
            "src=[Hãy tìm một bàn ở gần cuối], target=[Lets sit at one of the tables near the back], predicted=[a of of of of of tables near the home]\n",
            "src=[Điều đó tôi nghe chán rồi], target=[Im sick of hearing it], predicted=[i sick it hearing it]\n",
            "src=[Tôi có thể mượn điện thoại di động của bạn không], target=[Can I borrow your mobile phone], predicted=[can i borrow your mobile phone]\n",
            "src=[Anh hãy viết bằng bút mực], target=[Write with a pen], predicted=[write with a pen]\n",
            "BLEU-1: 0.540907\n",
            "BLEU-2: 0.438232\n",
            "BLEU-3: 0.396022\n",
            "BLEU-4: 0.291094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U0BPgvktPi2",
        "outputId": "606313d4-4a33-4d19-c0d8-6c329a1a215f"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-vie-both.pkl')\n",
        "train = load_clean_sentences('english-vie-train.pkl')\n",
        "test = load_clean_sentences('english-vie-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[Tôi phải làm bài tập], target=[I have homework to do], predicted=[i have homework do do]\n",
            "src=[Tom mất rất nhiều tiền], target=[Tom lost a lot of money], predicted=[tom is a lot of money]\n",
            "src=[Cuối cùng thì máy bay cũng cất cánh sau vài lần hoãn], target=[After several delays the plane finally left], predicted=[after several delays the plane finally left]\n",
            "src=[Và tại sao tôi phải làm điều đó], target=[And why would I do that], predicted=[what did you do do that]\n",
            "src=[Tôi không cho rằng anh ấy phù hợp với công việc đó], target=[I dont think he is fit for the job], predicted=[i dont think he was fit the the]\n",
            "src=[Sự tưởng tượng ảnh hưởng đến mọi khía cạnh cuộc sống của chúng ta], target=[Imagination affects every aspect of our lives], predicted=[imagination affects every aspect of our lives]\n",
            "src=[Anh biết Tom mà phải không], target=[You know about Tom dont you], predicted=[you you know tom you you]\n",
            "src=[Tom phải ở chỗ nào đó gần đây], target=[Tom has got to be here somewhere], predicted=[tom has have to to here somewhere]\n",
            "src=[Tôi không thể bỏ mặc cậu ở đó một mình], target=[I couldnt leave you there all by yourself], predicted=[i cant you you you all by yourself]\n",
            "src=[Thật không thể tin được], target=[Unbelievable], predicted=[unbelievable]\n",
            "BLEU-1: 0.651132\n",
            "BLEU-2: 0.545282\n",
            "BLEU-3: 0.488174\n",
            "BLEU-4: 0.367011\n",
            "test\n",
            "src=[Tom tới Paris để học tiếng Pháp], target=[Tom went to Paris to study French], predicted=[tom went to paris to study french]\n",
            "src=[Máy tính của tôi chạy kỳ lạ lắm], target=[My computers acting strange], predicted=[my computers acting strange]\n",
            "src=[Làm bất cứ điều gì mà bạn có thể], target=[Do whatever you can], predicted=[do whatever you can]\n",
            "src=[Tôi đang học nói tiếng Pháp], target=[Im learning to speak French], predicted=[im learning to french french]\n",
            "src=[Ai giàu ba họ ai khó ba đời], target=[Every dog has its day], predicted=[every dog has his day]\n",
            "src=[Tôi đang tìm một người bạn qua thư người Pháp], target=[Im looking for a French penpal], predicted=[im looking for a french penpal]\n",
            "src=[Mấy con gà đó hầu như ngày nào cũng đẻ trứng], target=[Those hens lay eggs almost every day], predicted=[those hens lay eggs almost every day]\n",
            "src=[Tôi biết là điều này có vẻ điên rồ nhưng tôi nghĩ là tôi đã yêu em gái của cậu mất rồi], target=[I realize that this may sound crazy but I think Ive fallen in love with your younger sister], predicted=[i realize this this may crazy crazy i think think ive in love with your your sister]\n",
            "src=[Tom đã rơi vào cái bẫy của Mary], target=[Tom has fallen for Marys trap], predicted=[tom has fallen for marys trap]\n",
            "src=[Tôi muốn nói chuyện với cô], target=[I want to talk to her], predicted=[i want to to to to]\n",
            "BLEU-1: 0.557776\n",
            "BLEU-2: 0.457328\n",
            "BLEU-3: 0.411586\n",
            "BLEU-4: 0.300803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxThAfV0jG_w",
        "outputId": "776a3824-62ac-4b3c-f068-870ec923a5b0"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-vie-both.pkl')\n",
        "train = load_clean_sentences('english-vie-train.pkl')\n",
        "test = load_clean_sentences('english-vie-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[Không cần giải thích], target=[Theres no need to explain], predicted=[theres no need to explain]\n",
            "src=[Cậu bé đã bắt con chim đó bằng một tấm lưới], target=[The boy captured the bird with a net], predicted=[the boy captured the bird with a net]\n",
            "src=[Có lẽ tôi sẽ gọi cho cậu lúc nào đó], target=[Maybe Ill call you sometime], predicted=[maybe ill call you sometime]\n",
            "src=[Tôi không muốn lấy chồng], target=[I dont want to get married], predicted=[i dont want to get married]\n",
            "src=[Con vua thì lại làm vua Con sãi ở chùa thì quét lá đa], target=[The apple doesnt fall far from the tree], predicted=[the apple like son fall far from the]\n",
            "src=[Bạn bị ốm rồi nghỉ ngơi cho nhiều đi], target=[Youre sick You have to rest], predicted=[youre sick you have to rest]\n",
            "src=[Muốn tôi làm gì thì anh cứ nói tôi sẽ làm cho], target=[Just tell me what you want me to do and Ill do it], predicted=[just tell me what you want me to do and ill do it]\n",
            "src=[Bạn không phải là một thợ máy nhỉ], target=[You arent a mechanic are you], predicted=[youre not a mechanic are you]\n",
            "src=[Chúng ta nên đi khi còn có thể], target=[We should leave while we still can], predicted=[we should leave while we still can]\n",
            "src=[Túi bạn là cái nào], target=[Which one is your bag], predicted=[which one is your bag]\n",
            "BLEU-1: 0.781726\n",
            "BLEU-2: 0.738286\n",
            "BLEU-3: 0.720109\n",
            "BLEU-4: 0.647246\n",
            "test\n",
            "src=[Ấy có sợ chó không], target=[Are you afraid of dogs], predicted=[are you afraid of dogs]\n",
            "src=[Tom cầu xin Mary cho anh ấy một cơ hội khác], target=[Tom pleaded with Mary to give him another chance], predicted=[tom pleaded with mary to give him another chance]\n",
            "src=[Nhân tiện nói về Kyoto bạn đã đi chùa Kinkakuji bao giờ chưa], target=[Speaking of Kyoto have you ever visited the Kinkakuji Temple], predicted=[speaking of kyoto have you ever visited the kinkakuji temple]\n",
            "src=[Chắc hẳn là Tom đang rất tự hào], target=[Tom must be so proud], predicted=[tom must be so proud]\n",
            "src=[Tom đã thắp nến lên], target=[Tom lit the candles], predicted=[tom lit the candles]\n",
            "src=[Tôi tự hỏi liệu lớp của Tom có bao nhiêu học sinh], target=[I wonder how many students are in Toms class], predicted=[i wonder how many students are in toms class]\n",
            "src=[Tháng trước tôi không đến trường], target=[I didnt go to school last month], predicted=[i didnt go to school last month]\n",
            "src=[Bạn có cái nào nhỏ hơn cái đó không], target=[Dont you have anything smaller than that], predicted=[dont you have anything smaller than that]\n",
            "src=[Nhắc anh ấy về nhà sớm nhé], target=[Remind him to come home early], predicted=[remind him to come home early]\n",
            "src=[Nhấp thử một ngụm đi], target=[Take a sip of this], predicted=[take a sip of this]\n",
            "BLEU-1: 0.664341\n",
            "BLEU-2: 0.613657\n",
            "BLEU-3: 0.602967\n",
            "BLEU-4: 0.528193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqYYAs-GlS-p",
        "outputId": "c8456e6d-66f1-4614-e1be-2babec0dacc6"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 100:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-vie-both.pkl')\n",
        "train = load_clean_sentences('english-vie-train.pkl')\n",
        "test = load_clean_sentences('english-vie-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[Không cần giải thích], target=[Theres no need to explain], predicted=[theres no need to explain]\n",
            "src=[Cậu bé đã bắt con chim đó bằng một tấm lưới], target=[The boy captured the bird with a net], predicted=[the boy captured the bird with a net]\n",
            "src=[Có lẽ tôi sẽ gọi cho cậu lúc nào đó], target=[Maybe Ill call you sometime], predicted=[maybe ill call you sometime]\n",
            "src=[Tôi không muốn lấy chồng], target=[I dont want to get married], predicted=[i dont want to get married]\n",
            "src=[Con vua thì lại làm vua Con sãi ở chùa thì quét lá đa], target=[The apple doesnt fall far from the tree], predicted=[the apple like son fall far from the]\n",
            "src=[Bạn bị ốm rồi nghỉ ngơi cho nhiều đi], target=[Youre sick You have to rest], predicted=[youre sick you have to rest]\n",
            "src=[Muốn tôi làm gì thì anh cứ nói tôi sẽ làm cho], target=[Just tell me what you want me to do and Ill do it], predicted=[just tell me what you want me to do and ill do it]\n",
            "src=[Bạn không phải là một thợ máy nhỉ], target=[You arent a mechanic are you], predicted=[youre not a mechanic are you]\n",
            "src=[Chúng ta nên đi khi còn có thể], target=[We should leave while we still can], predicted=[we should leave while we still can]\n",
            "src=[Túi bạn là cái nào], target=[Which one is your bag], predicted=[which one is your bag]\n",
            "src=[Tôi không biết cả hai thằng], target=[I dont know either boy], predicted=[i dont know either boy]\n",
            "src=[Vài người có thói quen phàn nàn về mọi việc], target=[Some people seem to complain about everything], predicted=[some people seem to complain about everything]\n",
            "src=[Nó lùn nhưng rất giỏi môn bóng rổ], target=[He is short but good at basketball], predicted=[he is short but good at basketball]\n",
            "src=[Tôi bỗng dưng thèm ăn da gà nướng giòn], target=[I suddenly got a craving to eat some crisp roast chicken skin], predicted=[i suddenly got a craving to eat some crisp roast chicken skin]\n",
            "src=[Cậu ta tập chơi đàn hàng ngày để trở thành một nghệ sỹ dương cầm], target=[He practices the piano every day so he can become a pianist], predicted=[he practices the piano every day so he can become a pianist]\n",
            "src=[Chúng tôi không nghĩ là họ sẽ mua một chiếc ô tô mới], target=[We do not anticipate their buying a new car], predicted=[we do not anticipate their buying a new car]\n",
            "src=[Bọn tớ chả ăn đồ ăn này bao giờ], target=[Were not used this kind of food], predicted=[were not used this kind of food]\n",
            "src=[Tom thiếu tài năng để trở thành một diễn viên], target=[Tom lacks the talent to be an actor], predicted=[tom lacks the talent to be an actor]\n",
            "src=[Mùa đông là mùa tôi thích nhất], target=[Winter is my favorite season], predicted=[winter is my favorite season]\n",
            "src=[Rất nhiều người không nhà không cửa], target=[Many people had no homes at all], predicted=[many people had no homes at all]\n",
            "src=[Anh ta biết đánh ghita], target=[He can play the guitar], predicted=[he is able the play the guitar]\n",
            "src=[Tôi phải viết một lá thư Bạn có tờ giấy nào không], target=[I have to write a letter Do you have some paper], predicted=[i have to write a letter do you have some paper]\n",
            "src=[Mai tôi sẽ ở đó], target=[Ill be there tomorrow], predicted=[ill be there tomorrow]\n",
            "src=[Bạn có khát nước không], target=[Are you thirsty], predicted=[are you thirsty]\n",
            "src=[Tôi đang học lớp tám], target=[Im an eighth grader], predicted=[im an eighth grader]\n",
            "src=[Tao biết tỏng trò của mày rồi], target=[I know what your game is], predicted=[i know what your game is]\n",
            "src=[Tôi có một tuổi thơ hạnh phúc], target=[I had a happy childhood], predicted=[i had a happy childhood]\n",
            "src=[Tôi sẽ chơi tennis với Tom lúc chiều muộn], target=[Ill play tennis with Tom later this afternoon], predicted=[ill play tennis with tom later this afternoon]\n",
            "src=[Một số người không thích lươn vì trông bọn nó giống rắn], target=[Some people dislike eels because they look like snakes], predicted=[some people dislike eels because they look like snakes]\n",
            "src=[Chiếc xe này là của hắn], target=[This car is his], predicted=[this car is his]\n",
            "src=[Anh ấy đã nghỉ học vì bị ốm], target=[He was absent from school because he was sick], predicted=[he was absent from school because he was sick]\n",
            "src=[Anh ta bị phạt tiền vì đỗ xe trái phép], target=[He was fined for illegal parking], predicted=[he was fined for illegal parking]\n",
            "src=[Tôi đã nghĩ là Tom sẽ trồng những cây hóa đó gần cây sồi], target=[I thought Tom would plant those flowers near the oak tree], predicted=[i thought tom would plant those flowers near the oak tree]\n",
            "src=[Tôi đã ngay lập tức gọi cho cô ấy], target=[I telephoned her at once], predicted=[i telephoned her at once]\n",
            "src=[Thế giới không quay quanh mày], target=[The world doesnt revolve around you], predicted=[the world doesnt revolve around you]\n",
            "src=[Tại sao Tom lại có thể nói tiếng Pháp giỏi đến vậy], target=[Why can Tom speak French so well], predicted=[why can tom speak french so well]\n",
            "src=[An toàn là trên hết], target=[You should put safety before everything else], predicted=[you should put safety before everything else]\n",
            "src=[Người đàn ông bị truy nã về tội giết người], target=[The man is wanted for murder], predicted=[the man is wanted for murder]\n",
            "src=[Bạn là một người chơi quần vợt giỏi], target=[Youre a very good tennis player], predicted=[youre a very good tennis player]\n",
            "src=[Tom đã tin Mary vô tội], target=[Tom believed in Marys innocence], predicted=[tom believed in marys innocence]\n",
            "src=[Tom mới đang tập bò thôi], target=[Tom is just learning to crawl], predicted=[tom is just learning to crawl]\n",
            "src=[Tom đã lại thắng một lần nữa], target=[Tom won again], predicted=[tom won again]\n",
            "src=[Bạn có bằng lái xe không], target=[Do you have a drivers license], predicted=[do you have a drivers license]\n",
            "src=[Tôi đã thay avatar của mình], target=[I changed my profile picture], predicted=[i changed my profile picture]\n",
            "src=[Tôi muốn cải thiện phát âm tiếng Pháp của tôi], target=[I would like to improve my French pronunciation], predicted=[i would like to improve my french pronunciation]\n",
            "src=[Bạn đã nói dối chúng tôi], target=[You lied to us], predicted=[you lied to us]\n",
            "src=[Cặp vợ chồng quyết định nhận một đứa con nuôi], target=[The couple decided to adopt an orphan], predicted=[the couple decided to adopt an orphan]\n",
            "src=[Chỉ còn một việc cần làm], target=[There is only one thing to do], predicted=[there is only one thing to do]\n",
            "src=[Tom muốn ăn xănguých], target=[Tom would like a sandwich], predicted=[tom would like a sandwich]\n",
            "src=[Tôi mong đợi đến ngày mai quá], target=[Im looking forward to tomorrow], predicted=[i looking hardly to tomorrow tomorrow]\n",
            "src=[Anh đã tìm thấy cái ví này ở đâu], target=[Where did you find this wallet], predicted=[where did you find this wallet]\n",
            "src=[Chúng tôi đến để giúp], target=[We came to help], predicted=[we came to help]\n",
            "src=[Cuốn sách của cô ấy rất thú vị], target=[Her book is very interesting], predicted=[her book is very interesting]\n",
            "src=[Bạn có biết cách dùng từ điển không], target=[Do you know how to use a dictionary], predicted=[do you know how to use a dictionary]\n",
            "src=[Viết bằng tay trái của bạn], target=[Write with your left hand], predicted=[write with your left hand]\n",
            "src=[Tom mất rất nhiều tiền], target=[Tom lost a lot of money], predicted=[tom lost a lot of money]\n",
            "src=[Tôi sẽ đón bạn vào ngày mai sau khi xong việc], target=[Ill pick you up tomorrow after work], predicted=[ill pick you up tomorrow after work]\n",
            "src=[Máy tính của Tom bị đứng máy], target=[Toms computer isnt responding], predicted=[toms computer isnt responding]\n",
            "src=[Tôi không biết là bạn có thể giúp tôi làm điều đó không], target=[I wonder if you could help me do that], predicted=[i wonder if you could help me do that]\n",
            "src=[Không phải lúc nào người già cũng khôn ngoan hơn người trẻ], target=[Old people arent always wiser than young people], predicted=[old people are always always wiser young the young]\n",
            "src=[Tom là một người Anh điển hình], target=[Toms a typical Englishman], predicted=[toms a typical englishman]\n",
            "src=[Điều ước của bạn đã trở thành sự thật chưa], target=[Did you get your wish], predicted=[did you get your wish]\n",
            "src=[Điều này sẽ không bao giờ chấm dứt], target=[This is never going to end], predicted=[this is never going to end]\n",
            "src=[Bạn có cần giúp gì không], target=[Do you need help with something], predicted=[do you need help with something]\n",
            "src=[Bạn thấy người phụ nữ đó ở đâu], target=[Where did you see that woman], predicted=[where did you see that woman]\n",
            "src=[Bạn nói tiếng gì], target=[What languages do you speak], predicted=[what languages do you speak]\n",
            "src=[Tom đang cố giết tôi], target=[Tom is trying to kill me], predicted=[tom is trying to kill me]\n",
            "src=[Tôi không tin con mèo đen đó mang đến điều xui xẻo], target=[I dont believe that black cats cause bad luck], predicted=[i dont believe that black cats cause bad luck]\n",
            "src=[Hãy đi ngủ một chút], target=[Lets get some sleep], predicted=[lets get some sleep]\n",
            "src=[Tôi cũng biết cưỡi ngựa], target=[I also know how to ride a horse], predicted=[i also know how to ride a horse]\n",
            "src=[Tôi không biết nói tiếng Nhật], target=[I dont speak Japanese], predicted=[i dont speak japanese]\n",
            "src=[Chính xác là bạn đã làm gì], target=[What did you do exactly], predicted=[what did you do exactly]\n",
            "src=[Tôi muốn sử dụng thời gian tốt hơn quá], target=[I want to become better at managing my time], predicted=[i want to become better at managing my time]\n",
            "src=[Anh ta xác nhận rằng đó là xác của Titanic], target=[He confirmed that it was the wreck of the Titanic], predicted=[he confirmed that it was the wreck of the titanic]\n",
            "src=[Tự do không đồng nghĩa với được tự do], target=[Freedom is not free], predicted=[freedom is not free]\n",
            "src=[Tom đã hỏi tôi có biết ai có thể dịch từ tiếng Pháp sang tiếng Anh không], target=[Tom asked me if I knew anybody who could translate from French into English], predicted=[tom asked me if i knew anybody who could translate translate french into english]\n",
            "src=[Bạn đã sẵn sàng cho mùa xuân chưa], target=[Are you ready for spring], predicted=[are you ready for spring]\n",
            "src=[Cô ấy thực sự dễ thương], target=[She is really cute], predicted=[she is really cute]\n",
            "src=[Tom chắc chắn sẽ không hài lòng nếu Mary quyết định quay trở lại làm việc], target=[Tom certainly wouldnt be pleased if Mary decided to go back to work], predicted=[tom certainly wouldnt be pleased if mary decided to go back to work]\n",
            "src=[Thử xem nào], target=[Lets have a try], predicted=[lets have a try]\n",
            "src=[Chúng ta không có nhiều sự lựa chọn phải không], target=[We dont have much choice do we], predicted=[we dont have much choice do we]\n",
            "src=[Tom hơi giận Mary], target=[Tom is a little angry at Mary], predicted=[tom is a little angry at mary]\n",
            "src=[Bạn có thực sự muốn giúp], target=[Do you really want to help], predicted=[do you really want to help]\n",
            "src=[Từ xa tôi đã nghe thấy tiếng sét], target=[I heard it thunder in the distance], predicted=[i heard it thunder in the distance]\n",
            "src=[Tôi có thể nói với Tom bất cứ điều gì], target=[I can tell Tom anything], predicted=[i can tell tom anything]\n",
            "src=[Đây là ngôi nhà nơi mà anh ấy lớn lên], target=[This is the house where he was brought up], predicted=[this is the house where he was brought up]\n",
            "src=[Tom là một ca sĩ dân ca], target=[Tom is a folk singer], predicted=[tom is a folk singer]\n",
            "src=[Cảnh sát kêu gọi đám đông đừng hoảng hốt], target=[The police appealed to the crowd not to panic], predicted=[the police appealed to the crowd not to panic]\n",
            "src=[Ý bạn là sao], target=[What are you implying], predicted=[what are you getting at]\n",
            "src=[Tôi sẽ không dừng lại], target=[Im not going to stop], predicted=[im not going to stop]\n",
            "src=[Tom thích nó lắm], target=[Tom loves it], predicted=[tom loves it]\n",
            "src=[Đừng quên đem theo máy ảnh], target=[Dont forget to bring a camera], predicted=[dont forget to bring a camera]\n",
            "src=[Hãy giúp tôi việc đó], target=[Help me to do it], predicted=[help me to do it]\n",
            "src=[Có vẻ như hôm nay trời sẽ mưa], target=[It looks like itll rain today], predicted=[it looks like itll rain today]\n",
            "src=[Anh ta đi ra ngoài bất chấp cơn mưa], target=[He went out in spite of the rain], predicted=[he went out in spite of the rain]\n",
            "src=[Bạn cần phải học hỏi từ sai lầm của bản thân], target=[You must learn from your mistakes], predicted=[you must learn from your mistakes]\n",
            "src=[Tôi chưa từng thấy Tom bận như thế], target=[Ive never seen Tom so busy], predicted=[ive never seen tom so busy]\n",
            "src=[Có nhiều sinh viên trong thư viện], target=[There are students in the library], predicted=[there are students in the library]\n",
            "src=[Tôi đã bất ngờ khi thấy bạn thắng giải đấy], target=[I was surprised that you won the prize], predicted=[i was surprised that you won the prize]\n",
            "src=[Tôi phải làm bài tập], target=[I have homework to do], predicted=[i have homework to do]\n",
            "BLEU-1: 0.781726\n",
            "BLEU-2: 0.738286\n",
            "BLEU-3: 0.720109\n",
            "BLEU-4: 0.647246\n",
            "test\n",
            "src=[Ấy có sợ chó không], target=[Are you afraid of dogs], predicted=[are you afraid of dogs]\n",
            "src=[Tom cầu xin Mary cho anh ấy một cơ hội khác], target=[Tom pleaded with Mary to give him another chance], predicted=[tom pleaded with mary to give him another chance]\n",
            "src=[Nhân tiện nói về Kyoto bạn đã đi chùa Kinkakuji bao giờ chưa], target=[Speaking of Kyoto have you ever visited the Kinkakuji Temple], predicted=[speaking of kyoto have you ever visited the kinkakuji temple]\n",
            "src=[Chắc hẳn là Tom đang rất tự hào], target=[Tom must be so proud], predicted=[tom must be so proud]\n",
            "src=[Tom đã thắp nến lên], target=[Tom lit the candles], predicted=[tom lit the candles]\n",
            "src=[Tôi tự hỏi liệu lớp của Tom có bao nhiêu học sinh], target=[I wonder how many students are in Toms class], predicted=[i wonder how many students are in toms class]\n",
            "src=[Tháng trước tôi không đến trường], target=[I didnt go to school last month], predicted=[i didnt go to school last month]\n",
            "src=[Bạn có cái nào nhỏ hơn cái đó không], target=[Dont you have anything smaller than that], predicted=[dont you have anything smaller than that]\n",
            "src=[Nhắc anh ấy về nhà sớm nhé], target=[Remind him to come home early], predicted=[remind him to come home early]\n",
            "src=[Nhấp thử một ngụm đi], target=[Take a sip of this], predicted=[take a sip of this]\n",
            "src=[Họ chả biết gì cả], target=[They dont know anything], predicted=[they dont know anything]\n",
            "src=[Tôi cần thêm], target=[I need more], predicted=[i need more]\n",
            "src=[Chuyện gì đã qua thì cho qua], target=[It is no use crying over spilt milk], predicted=[there is no use crying over spilt milk]\n",
            "src=[Bạn chạy], target=[You run], predicted=[you run]\n",
            "src=[Anh ấy đã quyên góp 10 nghìn Đôla cho quỹ hỗ trợ người tị nạn], target=[He donated 10000 to the refugee fund], predicted=[he donated 10000 to the refugee fund]\n",
            "src=[Lúc đó Tom quét sàn phòng bếp trong khi Mary rửa bát], target=[Tom swept the kitchen floor while Mary washed the dishes], predicted=[tom swept the kitchen floor while mary washed the dishes]\n",
            "src=[Tôi rất thích món ăn Hàn Quốc], target=[I love Korean food], predicted=[i love korean food]\n",
            "src=[Tom có hai người em trai], target=[Tom has two brothers], predicted=[tom has two brothers]\n",
            "src=[Trong hai tuần nữa bạn sẽ có thể ra viện], target=[In another two weeks you will be able to get out of the hospital], predicted=[in another two weeks you will be able to get out of the hospital]\n",
            "src=[Thông thường khi Tom và Mary ra ngoài ăn tối cùng nhau họ cùng trả tiền hoá đơn], target=[Usually when Tom and Mary go out for dinner together they split the bill], predicted=[usually when tom and mary go out for dinner together they split the bill]\n",
            "src=[Tôi đã trả cho anh ấy 5 Đôla], target=[I paid him five dollars], predicted=[i paid him five dollars]\n",
            "src=[Tôi biết là điều này có vẻ điên rồ nhưng tôi nghĩ là tôi đã yêu em gái của cậu mất rồi], target=[I realize this may sound crazy but I think Ive fallen in love with your younger sister], predicted=[i realize that this sound sound but but i think ive fallen in love with younger younger]\n",
            "src=[Tôi không muốn bất kỳ ai viết về tôi], target=[I dont want anybody writing about me], predicted=[i dont want anybody writing about me]\n",
            "src=[Mẹ đã mua cho chúng tôi một con cún], target=[Mother bought us a puppy], predicted=[mother bought us a puppy]\n",
            "src=[Tôi không thể diễn tả bằng lời rằng tôi biết ơn sự giúp đỡ của bạn đến nhường nào], target=[I cant tell you how much I appreciate all your help], predicted=[i cant tell you how much i appreciate all your help]\n",
            "src=[Tôi chuyên về lịch sử thời Trung cổ], target=[I specialize in medieval history], predicted=[i specialize in medieval history]\n",
            "src=[Cô ấy thích nó], target=[She liked that], predicted=[she liked that]\n",
            "src=[Tôi là chủ sở hữu của tòa nhà này], target=[Im the owner of this building], predicted=[im the owner of this building]\n",
            "src=[Tôi không phải là Tom], target=[Im not Tom], predicted=[im not tom]\n",
            "src=[Tôi đã đến thăm ngôi làng nơi anh ấy được sinh ra], target=[I visited the village where he was born], predicted=[i visited the village where he was born]\n",
            "src=[Chúng tôi đã có một trải nghiệm không mấy dễ chịu ở đó], target=[We had an unpleasant experience there], predicted=[we had an unpleasant experience there]\n",
            "src=[Tôi chỉ thấy hơi chóng mặt một tí], target=[I just feel a little dizzy], predicted=[i just feel a little dizzy]\n",
            "src=[Tôi còn có sự chọn lựa nào khác nữa], target=[What other options do I have], predicted=[what other options do i have]\n",
            "src=[Bạn có đôi chân rất sexy], target=[You have very sexy legs], predicted=[you have very sexy legs]\n",
            "src=[Tôi nghĩ là chúng ta cần phải tìm cho ra là Tom định đưa nó cho ai], target=[I think we need to find out who Tom plans to give that to], predicted=[i think we need to find out who tom to give give that]\n",
            "src=[Tôi đã quyết định là kể từ lúc này tôi sẽ học chăm chỉ hơn], target=[Ive made up my mind to study harder from now on], predicted=[ive made up my mind to study harder from now on]\n",
            "src=[Tôi đang nóng], target=[Im getting hot], predicted=[im getting hot]\n",
            "src=[Cô ta từ chối mọi lời đề nghị], target=[She turned down every proposal], predicted=[she turned down every proposal]\n",
            "src=[Thầy vật lý không để ý đến việc lên lớp của tôi], target=[My physics teacher doesnt care if I skip classes], predicted=[my physics teacher doesnt care if i skip classes]\n",
            "src=[Vấn đề lớn nhất của tôi là quyết định làm cái gì kế tiếp], target=[My biggest problem is deciding what to do next], predicted=[my biggest problem is deciding what to do next]\n",
            "src=[Chúc mừng giáng sinh], target=[Merry Christmas], predicted=[merry christmas]\n",
            "src=[Tôi mong sao Tom đừng hát quá to lúc đêm khuya], target=[I wish Tom wouldnt sing so loudly late at night], predicted=[i wish tom wouldnt sing so loudly late at night]\n",
            "src=[Đi qua cái cửa màu cam], target=[Go through the orange door], predicted=[go through the orange door]\n",
            "src=[Cô ấy thích nó], target=[She liked it], predicted=[she liked that]\n",
            "src=[Giúp đỡ Tom là một sai lầm], target=[Helping Tom was a mistake], predicted=[helping tom was a mistake]\n",
            "src=[Đừng quay lại đây], target=[Never come back here], predicted=[never come back here]\n",
            "src=[Các bạn có bao nhiêu cây vợt], target=[How many rackets do you have], predicted=[how many rackets do you have]\n",
            "src=[Tom để ý thấy cánh cửa chỉ khép hờ], target=[Tom noticed the door was half closed], predicted=[tom noticed the door was half closed]\n",
            "src=[Ở đó bây giờ là mấy giờ], target=[What time is it there], predicted=[what time is it there]\n",
            "src=[Bạn cho cha mẹ bạn xem cái đó chưa], target=[Did you show it to your parents], predicted=[did you show it to your parents]\n",
            "src=[Tom cần tìm kiếm sự giúp đỡ], target=[Tom needs to seek help], predicted=[tom needs to seek help]\n",
            "src=[Tôi đã quyết định không chống án], target=[Ive decided not to appeal], predicted=[ive decided not to appeal]\n",
            "src=[Tôi muốn quản lý thời gian tốt hơn quá], target=[I want to become better at managing my time], predicted=[i want to become better at managing my time]\n",
            "src=[Đã đến lúc trả đũa rồi], target=[Its payback time], predicted=[its payback time]\n",
            "src=[Bạn có nghĩ là bạn làm việc với chúng tôi được không], target=[Do you think youd like to work for us], predicted=[do you think youd like to work for us]\n",
            "src=[Tôi muốn đi du học ở Paris], target=[Id like to study in Paris], predicted=[id like to study in paris]\n",
            "src=[Nếu mệt thì ngủ một chút đi Bây giờ mà ngủ thì sẽ dậy sớm lắm], target=[If youre tired why dont you go to sleep Because if I go to sleep now I will wake up too early], predicted=[if youre tired why you you go to sleep because if i i to sleep sleep sleep sleep wake wake wake early early]\n",
            "src=[tao sẽ nhớ mày], target=[I will miss you], predicted=[i will miss you]\n",
            "src=[Ấy mua bộ đồ này ở đâu thế], target=[Where do you buy clothes], predicted=[where do you buy clothes]\n",
            "src=[Anh là của em], target=[Youre mine], predicted=[youre mine]\n",
            "src=[Tom nói là anh ấy tưởng Mary không sợ], target=[Tom said that he thought Mary wasnt afraid], predicted=[tom said that thought mary mary afraid afraid]\n",
            "src=[Tom nghe thấy tiếng ai đó huýt sáo ở bên ngoài], target=[Tom heard someone whistling outside], predicted=[tom heard someone whistling outside]\n",
            "src=[Tom nói là anh ta không biết đáp án], target=[Tom said he didnt know the answer], predicted=[tom said he didnt know the answer]\n",
            "src=[Lần tới tôi sẽ đến sớm hơn], target=[Next time Ill come earlier], predicted=[next time ill come earlier]\n",
            "src=[Cuốn sách này tôi mua ở hiệu sách trước nhà ga], target=[I bought this book at the book store in front of the station], predicted=[i bought this book at the book store in front of the station]\n",
            "src=[Tom cũng nhiều tuổi hơn tôi], target=[Tom is older than me too], predicted=[tom is older than me too]\n",
            "src=[Cô ấy là mẫu người của tôi], target=[Shes my type], predicted=[shes my type]\n",
            "src=[Tôi đã gọi Tom bạn thân của tôi], target=[I called my good friend Tom], predicted=[i called my good friend tom]\n",
            "src=[Tất cả quả bóng đều có màu vàng], target=[All of the balls are yellow], predicted=[all of the balls are yellow]\n",
            "src=[Tom không cho Mary cơ hội để giải thích], target=[Tom didnt give Mary a chance to explain], predicted=[tom didnt give mary a chance to explain]\n",
            "src=[Bây giờ tôi mở mắt được chưa], target=[Can I open my eyes now], predicted=[can i open my eyes now]\n",
            "src=[Tôi đã nhấp thêm một ngụm bia to], target=[I took another swig of beer], predicted=[i took another swig of beer]\n",
            "src=[Chị ấy không có bằng lái xe], target=[She doesnt have a drivers license], predicted=[she doesnt have a drivers license]\n",
            "src=[Tôi rất thích quả cam], target=[I like oranges a lot], predicted=[i like oranges a much]\n",
            "src=[Bọn mình không nhận được đồ ăn hay nước uống], target=[We werent given any food or water], predicted=[we werent given any food or water]\n",
            "src=[Tom đã nghe thấy ai đó gọi tên mình], target=[Tom heard his name being called], predicted=[tom heard his name being called]\n",
            "src=[Tom đã bị choáng], target=[Tom was stunned], predicted=[tom was stunned]\n",
            "src=[Thanh toán bằng Visa được chứ], target=[Can I pay by Visa], predicted=[can i pay by visa]\n",
            "src=[Cô ta đã bảo các cậu trai sơn ngôi nhà], target=[She had the boys paint the house], predicted=[she had the boys paint the house]\n",
            "src=[Bạn có ý nào tốt hơn không], target=[Can you think of a better idea than this one], predicted=[can you think of a better idea than this this]\n",
            "src=[Chúng tôi đã ngồi đây gần một tiếng rồi], target=[Weve been sitting here for almost an hour], predicted=[weve been sitting here for almost an hour]\n",
            "src=[Nếu tôi có tiền tôi sẽ mua máy vi tính], target=[If I had money Id buy a computer], predicted=[if i had money id buy a computer]\n",
            "src=[Hình như Tom không hiểu những điều bạn nói], target=[Tom didnt seem to understand what you said], predicted=[tom didnt seem to understand what you said]\n",
            "src=[Tại sao mọi người lại nói dối], target=[Why do people tell lies], predicted=[why do people tell lies]\n",
            "src=[Nhiều người nghĩ là tôi điên], target=[Most people think Im crazy], predicted=[most people think im crazy]\n",
            "src=[Tại sao mọi người nói dối], target=[Why do people tell lies], predicted=[why do people tell lies]\n",
            "src=[Mình không rảnh], target=[Im not free], predicted=[im not free]\n",
            "src=[Tất cả chúng tôi cùng đứng dậy], target=[We all stood up at once], predicted=[we all stood up at once]\n",
            "src=[Tôi có ước mơ], target=[I have a dream], predicted=[i have a dream]\n",
            "src=[Bạn có thể dùng cái laptop của tôi nếu bạn muốn], target=[You may use my computer if you want to], predicted=[you may use my computer if you want to]\n",
            "src=[Hôm nay tôi không thể làm việc], target=[I cant work today], predicted=[i cant work today]\n",
            "src=[Tom vẫn cố làm cho dù cậu ấy rất mệt], target=[Tom kept working even though he was very tired], predicted=[tom kept working even though he was very tired]\n",
            "src=[Tôi cần làm điều đó], target=[Its become necessary for me to do that], predicted=[its become necessary for me to do that]\n",
            "src=[Mày có mấy cây vợt], target=[How many rackets do you have], predicted=[how many rackets do you have]\n",
            "src=[Tôi sẽ làm những điều mà tôi đã nói], target=[Im going to do what Ive been told], predicted=[im going to do what ive been told]\n",
            "src=[Hôm nay là một ngày ấm áp], target=[Today is a warm day], predicted=[today is a warm day]\n",
            "src=[Người ta bảo khi còn trẻ ông ta rất nghèo], target=[He is said to have been very poor when he was young], predicted=[he is said to have been very poor when he was young]\n",
            "src=[Họ định di cư sang Mỹ], target=[They are going to emigrate to the United States], predicted=[they are going to emigrate to the united states]\n",
            "src=[Đó có phải là người đàn ông đã mất vợ trong vụ tai nạn đó không], target=[Is that the man whose wife was killed in the car accident], predicted=[is that the man whose wife was killed in the car accident]\n",
            "src=[Cuối cùng tôi có thời gian để trả lời thư mà tôi đã nhận trong 3 tuần qua], target=[Finally I have time to reply to the mail that I have received these past three weeks], predicted=[finally i have time to reply to the mail that have have received these past three weeks]\n",
            "BLEU-1: 0.664341\n",
            "BLEU-2: 0.613657\n",
            "BLEU-3: 0.602967\n",
            "BLEU-4: 0.528193\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}